{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## tutorial to better understand how mutual information, common for decision trees\n",
    "## tutorial url:\n",
    "## https://machinelearningmastery.com/information-gain-and-mutual-information/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain calculates the reduction in entropy or surprise from transforming a dataset in some way.\n",
    "\n",
    "It is commonly used in the construction of decision trees from a training dataset, by evaluating the information gain for each variable, and selecting the variable that maximizes the information gain, which in turn minimizes the entropy and best splits the dataset into groups for effective classification.\n",
    "\n",
    "Information gain can also be used for feature selection, by evaluating the gain of each variable in the context of the target variable. In this slightly different usage, the calculation is referred to as mutual information between the two random variables."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain is calculated by comparing the entropy of the dataset before and after a transformation.\n",
    "\n",
    "Mutual information calculates the statistical dependence between two variables and is the name given to information gain when applied to variable selection."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "You might recall that information quantifies how surprising an event is in bits. Lower probability events have more information, higher probability events have less information. Entropy quantifies how much information there is in a random variable, or more specifically its probability distribution. A skewed distribution has a low entropy, whereas a distribution where events have equal probability has a larger entropy.\n",
    "\n",
    "In information theory, we like to describe the “surprise” of an event. Low probability events are more surprising therefore have a larger amount of information. Whereas probability distributions where the events are equally likely are more surprising and have larger entropy.\n",
    "\n",
    "Skewed Probability Distribution (unsurprising): Low entropy.\n",
    "\n",
    "Balanced Probability Distribution (surprising): High entropy."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy: 0.469 bits\n"
     ]
    }
   ],
   "source": [
    "# calculate the entropy for a dataset\n",
    "from math import log2\n",
    "# proportion of examples in each class\n",
    "class0 = 10/100\n",
    "class1 = 90/100\n",
    "# calculate entropy\n",
    "entropy = -(class0 * log2(class0) + class1 * log2(class1))\n",
    "# print the result\n",
    "print('entropy: %.3f bits' % entropy)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "entropy: 1.000 bits\n"
     ]
    }
   ],
   "source": [
    "# proportion of examples in each class\n",
    "class0 = 50/100\n",
    "class1 = 50/100\n",
    "# calculate entropy\n",
    "entropy = -(class0 * log2(class0) + class1 * log2(class1))\n",
    "# print the result\n",
    "print('entropy: %.3f bits' % entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Information gain can be calculated as follows:\n",
    "\n",
    "IG(S, a) = H(S) – H(S | a)\n",
    "Where IG(S, a) is the information for the dataset S for the variable a for a random variable, H(S) is the entropy for the dataset before any change (described above) and H(S | a) is the conditional entropy for the dataset given the variable a.\n",
    "\n",
    "This calculation describes the gain in the dataset S for the variable a. It is the number of bits saved when transforming the dataset."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Worked Example of Calculating Information Gain"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We can define a function to calculate the entropy of a group of samples based on the ratio of samples that belong to class 0 and class 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "def entropy(class0, class1):\n",
    "    return -(class0 * log2(class0) + class1 * log2(class1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, consider a dataset with 20 examples, 13 for class 0 and 7 for class 1. We can calculate the entropy for this dataset, which will have less than 1 bit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dataset Entropy: 0.934 bits\n"
     ]
    }
   ],
   "source": [
    "# split of the main dataset\n",
    "class0 = 13 / 20\n",
    "class1 = 7 / 20\n",
    "# calculate entropy before the change\n",
    "s_entropy = entropy(class0, class1)\n",
    "print('Dataset Entropy: %.3f bits' % s_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now consider that one of the variables in the dataset has two unique values, say “value1” and “value2.” We are interested in calculating the information gain of this variable.\n",
    "\n",
    "Let’s assume that if we split the dataset by value1, we have a group of eight samples, seven for class 0 and one for class 1. We can then calculate the entropy of this group of samples."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group1 Entropy: 0.544 bits\n"
     ]
    }
   ],
   "source": [
    "# split 1 (split via value1)\n",
    "s1_class0 = 7 / 8\n",
    "s1_class1 = 1 / 8\n",
    "# calculate the entropy of the first group\n",
    "s1_entropy = entropy(s1_class0, s1_class1)\n",
    "print('Group1 Entropy: %.3f bits' % s1_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, let’s assume that we split the dataset by value2; we have a group of 12 samples with six in each group. We would expect this group to have an entropy of 1."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Group2 Entropy: 1.000 bits\n"
     ]
    }
   ],
   "source": [
    "# split 2  (split via value2)\n",
    "s2_class0 = 6 / 12\n",
    "s2_class1 = 6 / 12\n",
    "# calculate the entropy of the second group\n",
    "s2_entropy = entropy(s2_class0, s2_class1)\n",
    "print('Group2 Entropy: %.3f bits' % s2_entropy)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this case, information gain can be calculated as:\n",
    "\n",
    "Entropy(Dataset) – (Count(Group1) / Count(Dataset) * Entropy(Group1) + Count(Group2) / Count(Dataset) * Entropy(Group2))\n",
    "Or:\n",
    "\n",
    "Entropy(13/20, 7/20) – (8/20 * Entropy(7/8, 1/8) + 12/20 * Entropy(6/12, 6/12))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Information Gain: 0.117 bits\n"
     ]
    }
   ],
   "source": [
    "# calculate the information gain\n",
    "gain = s_entropy - (8/20 * s1_entropy + 12/20 * s2_entropy)\n",
    "print('Information Gain: %.3f bits' % gain)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Examples of Information Gain in Machine Learning"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perhaps the most popular use of information gain in machine learning is in decision trees.\n",
    "\n",
    "\"Information gain is precisely the measure used by ID3 to select the best attribute at each step in growing the tree.\"\n",
    "\n",
    "The information gain is calculated for each variable in the dataset. The variable that has the largest information gain is selected to split the dataset. Generally, a larger gain indicates a smaller entropy or less surprise.\n",
    "\n",
    "The process is then repeated on each created group, excluding the variable that was already chosen. This stops once a desired depth to the decision tree is reached or no more splits are possible.\n",
    "\n",
    "Information gain can be used as a split criterion in most modern implementations of decision trees, such as the implementation of the Classification and Regression Tree (CART) algorithm in the scikit-learn Python machine learning library in the DecisionTreeClassifier class for classification."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import sklearn\n",
    "model = sklearn.tree.DecisionTreeClassifier(criterion='entropy')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## How Are Information Gain and Mutual Information Related?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Mutual Information and Information Gain are the same thing, although the context or usage of the measure often gives rise to the different names.\n",
    "\n",
    "For example:\n",
    "\n",
    "Effect of Transforms to a Dataset (decision trees): Information Gain.\n",
    "Dependence Between Variables (feature selection): Mutual Information.\n",
    "Notice the similarity in the way that the mutual information is calculated and the way that information gain is calculated; they are equivalent:\n",
    "\n",
    "I(X ; Y) = H(X) – H(X | Y)\n",
    "\n",
    "and\n",
    "\n",
    "IG(S, a) = H(S) – H(S | a)\n",
    "\n",
    "As such, mutual information is sometimes used as a synonym for information gain. Technically, they calculate the same quantity if applied to the same data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
