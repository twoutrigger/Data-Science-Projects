{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "## quick tutorial to break down the math behind precision, recall and F1 score\n",
    "## snippets from \"Approaching Almost Any Machine Learning Problem\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn import metrics"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def true_positive(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate True Positives\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: number of true positives\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    tp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 1:\n",
    "            tp += 1\n",
    "    return tp\n",
    "\n",
    "def true_negative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate True Negatives\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: number of true negatives\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    tn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 0:\n",
    "            tn += 1\n",
    "    return tn\n",
    "\n",
    "def false_positive(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate False Positives\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: number of false positives\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    fp = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 0 and yp == 1:\n",
    "            fp += 1\n",
    "    return fp\n",
    "\n",
    "def false_negative(y_true, y_pred):\n",
    "    \"\"\"\n",
    "    Function to calculate False Negatives\n",
    "    :param y_true: list of true values\n",
    "    :param y_pred: list of predicted values\n",
    "    :return: number of false negatives\n",
    "    \"\"\"\n",
    "    # initialize\n",
    "    fn = 0\n",
    "    for yt, yp in zip(y_true, y_pred):\n",
    "        if yt == 1 and yp == 0:\n",
    "            fn += 1\n",
    "    return fn"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Precision = TP / (TP + FP)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "def precision(y_true, y_pred):\n",
    " \"\"\"\n",
    " Function to calculate precision\n",
    " :param y_true: list of true values\n",
    " :param y_pred: list of predicted values\n",
    " :return: precision score\n",
    " \"\"\"\n",
    " tp = true_positive(y_true, y_pred)\n",
    " fp = false_positive(y_true, y_pred)\n",
    " precision = tp / (tp + fp)\n",
    " return precision"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6666666666666666"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "l1 = [0,1,1,1,0,0,0,1]\n",
    "l2 = [0,1,0,1,0,1,0,0]\n",
    "\n",
    "precision(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Recall = TP / (TP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def recall(y_true, y_pred):\n",
    " \"\"\"\n",
    " Function to calculate recall\n",
    " :param y_true: list of true values\n",
    " :param y_pred: list of predicted values\n",
    " :return: recall score\n",
    " \"\"\"\n",
    " tp = true_positive(y_true, y_pred)\n",
    " fn = false_negative(y_true, y_pred)\n",
    " recall = tp / (tp + fn)\n",
    " return recall"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    " recall(l1, l2)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### F1 Score = 2TP / (2TP + FP + FN)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "def f1(y_true, y_pred):\n",
    " \"\"\"\n",
    " Function to calculate f1 score\n",
    " :param y_true: list of true values\n",
    " :param y_pred: list of predicted values\n",
    " :return: f1 score\n",
    " \"\"\"\n",
    " p = precision(y_true, y_pred)\n",
    " r = recall(y_true, y_pred)\n",
    " score = 2 * p * r / (p + r)\n",
    " return score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5714285714285715"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "y_true = [0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "y_pred = [0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 1, 0, 0, 0, 0, 0, 0, 0, 1, 0]\n",
    "\n",
    "f1(y_true, y_pred)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.5714285714285715\n",
      "0.6666666666666666\n",
      "0.5\n"
     ]
    }
   ],
   "source": [
    "## sklearn implementations\n",
    "print(metrics.f1_score(y_true, y_pred))\n",
    "print(metrics.precision_score(l1, l2))\n",
    "print(metrics.recall_score(l1, l2))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Instead of looking at precision and recall individually, you can also just look at F1\n",
    "score. Same as for precision, recall and accuracy, F1 score also ranges from 0 to 1,\n",
    "and a perfect prediction model has an F1 of 1. When dealing with datasets that have\n",
    "skewed targets, we should look at F1 (or precision and recall) instead of accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
