{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "902dd652",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'C:\\\\Users\\\\liamk\\\\Documents\\\\spark\\\\spark-3.3.1-bin-hadoop3'"
      ]
     },
     "execution_count": 1,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import findspark\n",
    "findspark.init()\n",
    "findspark.find()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "4fa90e04",
   "metadata": {},
   "outputs": [],
   "source": [
    "from pyspark.sql import SparkSession\n",
    "from pyspark.sql.functions import *\n",
    "from pyspark.ml.feature import VectorAssembler\n",
    "from pyspark.ml.regression import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e1c2a6aa",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark = SparkSession.builder.appName('ml').getOrCreate()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "56374279",
   "metadata": {},
   "outputs": [],
   "source": [
    "path = '../Datasets/cogsley_sales.csv'\n",
    "\n",
    "data = spark.read.format(\"csv\")\\\n",
    "    .option(\"header\", \"true\")\\\n",
    "    .option(\"inferSchema\", \"true\")\\\n",
    "    .load(path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "05937eea",
   "metadata": {},
   "outputs": [],
   "source": [
    "data.cache() # cache data for faster reuse\n",
    "data = data.dropna()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "cbc7a4ec",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(RowID=1, OrderID=3, OrderDate=datetime.datetime(2010, 10, 13, 0, 0), OrderMonthYear=datetime.datetime(2010, 10, 1, 0, 0), Quantity=6, Quote=1200, DiscountPct=0.04, Rate=200, SaleAmount=1152.0, CustomerName='Muhammed MacIntyre', CompanyName='CA Inc.', Sector='Technology', Industry='Computer Software: Prepackaged Software', City='Highland Park', ZipCode=60035, State='Illinois', Region='Central', ProjectCompleteDate=datetime.datetime(2010, 10, 20, 0, 0), DaystoComplete=7, ProductKey='Development - Big Data', ProductCategory='Development', ProductSubCategory='Big Data', Consultant='Ethan Bird', Manager='Josh Martinez', HourlyWage=60, RowCount=1, WageMargin=0.7)"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "dbb79bb5",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RowID , IntegerType()\n",
      "OrderID , IntegerType()\n",
      "OrderDate , TimestampType()\n",
      "OrderMonthYear , TimestampType()\n",
      "Quantity , IntegerType()\n",
      "Quote , IntegerType()\n",
      "DiscountPct , DoubleType()\n",
      "Rate , IntegerType()\n",
      "SaleAmount , DoubleType()\n",
      "CustomerName , StringType()\n",
      "CompanyName , StringType()\n",
      "Sector , StringType()\n",
      "Industry , StringType()\n",
      "City , StringType()\n",
      "ZipCode , IntegerType()\n",
      "State , StringType()\n",
      "Region , StringType()\n",
      "ProjectCompleteDate , TimestampType()\n",
      "DaystoComplete , IntegerType()\n",
      "ProductKey , StringType()\n",
      "ProductCategory , StringType()\n",
      "ProductSubCategory , StringType()\n",
      "Consultant , StringType()\n",
      "Manager , StringType()\n",
      "HourlyWage , IntegerType()\n",
      "RowCount , IntegerType()\n",
      "WageMargin , DoubleType()\n"
     ]
    }
   ],
   "source": [
    "for field in data.schema.fields:\n",
    "    print(field.name +\" , \"+str(field.dataType))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "ed28fda4",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(OrderMonthYear=20090101, SaleAmount=741024.2000000001)"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "summary = data.select(\"OrderMonthYear\", \"SaleAmount\").groupby(\"OrderMonthYear\").sum()\\\n",
    "    .orderBy(\"OrderMonthYear\").toDF(\"OrderMonthYear\", \"SaleAmount\")\n",
    "\n",
    "results = summary.rdd.map(lambda r: (int(str(r.OrderMonthYear).replace('-', '')[:8]), r.SaleAmount))\\\n",
    "    .toDF([\"OrderMonthYear\", \"SaleAmount\"])\n",
    "\n",
    "results.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "id": "3b7c1f24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Row(features=DenseVector([20090101.0]), label=741024.2000000001)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "dat = results.select(\"OrderMonthYear\", \"SaleAmount\")\\\n",
    "    .rdd.map(lambda r: LabeledPoint(r[1], [r[0]]))\\\n",
    "    .toDF()\n",
    "\n",
    "dat.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "265742aa",
   "metadata": {},
   "source": [
    "## Fit linear models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "3072dfd3",
   "metadata": {},
   "outputs": [
    {
     "ename": "IllegalArgumentException",
     "evalue": "requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.mllib.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>.",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mIllegalArgumentException\u001b[0m                  Traceback (most recent call last)",
      "Input \u001b[1;32mIn [14]\u001b[0m, in \u001b[0;36m<cell line: 3>\u001b[1;34m()\u001b[0m\n\u001b[0;32m      1\u001b[0m lr \u001b[38;5;241m=\u001b[39m LinearRegression()\n\u001b[1;32m----> 3\u001b[0m modelA \u001b[38;5;241m=\u001b[39m \u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdat\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m{\u001b[49m\u001b[43mlr\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mregParam\u001b[49m\u001b[43m:\u001b[49m\u001b[38;5;241;43m0.0\u001b[39;49m\u001b[43m}\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m      4\u001b[0m modelB \u001b[38;5;241m=\u001b[39m lr\u001b[38;5;241m.\u001b[39mfit(dat, {lr\u001b[38;5;241m.\u001b[39mregParam:\u001b[38;5;241m100.0\u001b[39m})\n\u001b[0;32m      6\u001b[0m predA \u001b[38;5;241m=\u001b[39m modelA\u001b[38;5;241m.\u001b[39mtransform(dat)\n",
      "File \u001b[1;32m~\\Documents\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\ml\\base.py:203\u001b[0m, in \u001b[0;36mEstimator.fit\u001b[1;34m(self, dataset, params)\u001b[0m\n\u001b[0;32m    201\u001b[0m \u001b[38;5;28;01melif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(params, \u001b[38;5;28mdict\u001b[39m):\n\u001b[0;32m    202\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m params:\n\u001b[1;32m--> 203\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[43mparams\u001b[49m\u001b[43m)\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    204\u001b[0m     \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    205\u001b[0m         \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_fit(dataset)\n",
      "File \u001b[1;32m~\\Documents\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:383\u001b[0m, in \u001b[0;36mJavaEstimator._fit\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    382\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_fit\u001b[39m(\u001b[38;5;28mself\u001b[39m, dataset: DataFrame) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m JM:\n\u001b[1;32m--> 383\u001b[0m     java_model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_fit_java\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m    384\u001b[0m     model \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_create_model(java_model)\n\u001b[0;32m    385\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_copyValues(model)\n",
      "File \u001b[1;32m~\\Documents\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\ml\\wrapper.py:380\u001b[0m, in \u001b[0;36mJavaEstimator._fit_java\u001b[1;34m(self, dataset)\u001b[0m\n\u001b[0;32m    377\u001b[0m \u001b[38;5;28;01massert\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_java_obj \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[0;32m    379\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_transfer_params_to_java()\n\u001b[1;32m--> 380\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_java_obj\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfit\u001b[49m\u001b[43m(\u001b[49m\u001b[43mdataset\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_jdf\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[1;32m~\\Documents\\spark\\spark-3.3.1-bin-hadoop3\\python\\lib\\py4j-0.10.9.5-src.zip\\py4j\\java_gateway.py:1321\u001b[0m, in \u001b[0;36mJavaMember.__call__\u001b[1;34m(self, *args)\u001b[0m\n\u001b[0;32m   1315\u001b[0m command \u001b[38;5;241m=\u001b[39m proto\u001b[38;5;241m.\u001b[39mCALL_COMMAND_NAME \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1316\u001b[0m     \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mcommand_header \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1317\u001b[0m     args_command \u001b[38;5;241m+\u001b[39m\\\n\u001b[0;32m   1318\u001b[0m     proto\u001b[38;5;241m.\u001b[39mEND_COMMAND_PART\n\u001b[0;32m   1320\u001b[0m answer \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mgateway_client\u001b[38;5;241m.\u001b[39msend_command(command)\n\u001b[1;32m-> 1321\u001b[0m return_value \u001b[38;5;241m=\u001b[39m \u001b[43mget_return_value\u001b[49m\u001b[43m(\u001b[49m\n\u001b[0;32m   1322\u001b[0m \u001b[43m    \u001b[49m\u001b[43manswer\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mgateway_client\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mtarget_id\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mname\u001b[49m\u001b[43m)\u001b[49m\n\u001b[0;32m   1324\u001b[0m \u001b[38;5;28;01mfor\u001b[39;00m temp_arg \u001b[38;5;129;01min\u001b[39;00m temp_args:\n\u001b[0;32m   1325\u001b[0m     temp_arg\u001b[38;5;241m.\u001b[39m_detach()\n",
      "File \u001b[1;32m~\\Documents\\spark\\spark-3.3.1-bin-hadoop3\\python\\pyspark\\sql\\utils.py:196\u001b[0m, in \u001b[0;36mcapture_sql_exception.<locals>.deco\u001b[1;34m(*a, **kw)\u001b[0m\n\u001b[0;32m    192\u001b[0m converted \u001b[38;5;241m=\u001b[39m convert_exception(e\u001b[38;5;241m.\u001b[39mjava_exception)\n\u001b[0;32m    193\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(converted, UnknownException):\n\u001b[0;32m    194\u001b[0m     \u001b[38;5;66;03m# Hide where the exception came from that shows a non-Pythonic\u001b[39;00m\n\u001b[0;32m    195\u001b[0m     \u001b[38;5;66;03m# JVM exception message.\u001b[39;00m\n\u001b[1;32m--> 196\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m converted \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;28mNone\u001b[39m\n\u001b[0;32m    197\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m    198\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m\n",
      "\u001b[1;31mIllegalArgumentException\u001b[0m: requirement failed: Column features must be of type class org.apache.spark.ml.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>> but was actually class org.apache.spark.mllib.linalg.VectorUDT:struct<type:tinyint,size:int,indices:array<int>,values:array<double>>."
     ]
    }
   ],
   "source": [
    "lr = LinearRegression()\n",
    "\n",
    "modelA = lr.fit(dat, {lr.regParam:0.0})\n",
    "modelB = lr.fit(dat, {lr.regParam:100.0})\n",
    "\n",
    "predA = modelA.transform(dat)\n",
    "predB = modelB.transform(dat)\n",
    "\n",
    "display(predA)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "48f3d54b",
   "metadata": {},
   "outputs": [],
   "source": [
    "spark.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "da95efbb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff933c9b",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1ddf6e6f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pyspark\n",
    "import random\n",
    "sc = pyspark.SparkContext(appName=\"Pi\")\n",
    "num_samples = 100000000\n",
    "def inside(p):     \n",
    "  x, y = random.random(), random.random()\n",
    "  return x*x + y*y < 1\n",
    "count = sc.parallelize(range(0, num_samples)).filter(inside).count()\n",
    "pi = 4 * count / num_samples\n",
    "print(pi)\n",
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d874239",
   "metadata": {},
   "outputs": [],
   "source": [
    "sc.stop()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f3ab530",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
