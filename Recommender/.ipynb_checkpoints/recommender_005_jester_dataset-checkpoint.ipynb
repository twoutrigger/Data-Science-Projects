{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "## exploring the surprise package using the jester dataset, as opposed to the MovieLens dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import SVD\n",
    "from surprise import Dataset\n",
    "from surprise.model_selection import cross_validate"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Evaluating RMSE, MAE of algorithm SVD on 3 split(s).\n",
      "\n",
      "                  Fold 1  Fold 2  Fold 3  Mean    Std     \n",
      "RMSE (testset)    4.4254  4.4418  4.4299  4.4324  0.0069  \n",
      "MAE (testset)     3.3299  3.3424  3.3341  3.3355  0.0052  \n",
      "Fit time          79.80   79.47   79.82   79.70   0.16    \n",
      "Test time         6.23    6.09    5.56    5.96    0.29    \n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "{'test_rmse': array([4.42543513, 4.44175378, 4.42991528]),\n",
       " 'test_mae': array([3.3299064 , 3.34239317, 3.33406385]),\n",
       " 'fit_time': (79.7993631362915, 79.47046732902527, 79.82240152359009),\n",
       " 'test_time': (6.230987310409546, 6.089756965637207, 5.560126304626465)}"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data = Dataset.load_builtin('jester')\n",
    "\n",
    "# Use the famous SVD algorithm.\n",
    "algo = SVD()\n",
    "\n",
    "# Run 3-fold cross-validation and print results.\n",
    "cross_validate(algo, data, measures=['RMSE', 'MAE'], cv=3, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import accuracy\n",
    "from surprise.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "trainset, testset = train_test_split(data, test_size=.25)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "RMSE: 4.4857\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "4.485710111720141"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "algo = SVD()\n",
    "\n",
    "# Train the algorithm on the trainset, and predict ratings for the testset\n",
    "algo.fit(trainset)\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "# Then compute RMSE\n",
    "accuracy.rmse(predictions)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "from surprise import KNNBasic"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "## this snippet returns the following error message:\n",
    "## MemoryError: Unable to allocate 13.0 GiB for an array with shape (59132, 59132) and data type int32\n",
    "## Perhaps shrinking down the subspace could be a solution?\n",
    "\n",
    "# # Retrieve the trainset.\n",
    "# trainset = data.build_full_trainset()\n",
    "\n",
    "# # Build an algorithm, and train it.\n",
    "# algo = KNNBasic()\n",
    "# algo.fit(trainset)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from collections import defaultdict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_top_n(predictions, n=10):\n",
    "    \"\"\"Return the top-N recommendation for each user from a set of predictions.\n",
    "\n",
    "    Args:\n",
    "        predictions(list of Prediction objects): The list of predictions, as\n",
    "            returned by the test method of an algorithm.\n",
    "        n(int): The number of recommendation to output for each user. Default\n",
    "            is 10.\n",
    "\n",
    "    Returns:\n",
    "    A dict where keys are user (raw) ids and values are lists of tuples:\n",
    "        [(raw item id, rating estimation), ...] of size n.\n",
    "    \"\"\"\n",
    "\n",
    "    # First map the predictions to each user.\n",
    "    top_n = defaultdict(list)\n",
    "    for uid, iid, true_r, est, _ in predictions:\n",
    "        top_n[uid].append((iid, est))\n",
    "\n",
    "    # Then sort the predictions for each user and retrieve the k highest ones.\n",
    "    for uid, user_ratings in top_n.items():\n",
    "        user_ratings.sort(key=lambda x: x[1], reverse=True)\n",
    "        top_n[uid] = user_ratings[:n]\n",
    "\n",
    "    return top_n\n",
    "\n",
    "\n",
    "# First train an SVD algorithm on the movielens dataset.\n",
    "# data = Dataset.load_builtin('ml-100k')\n",
    "# trainset = data.build_full_trainset()\n",
    "algo = SVD()\n",
    "algo.fit(trainset)\n",
    "\n",
    "# Than predict ratings for all pairs (u, i) that are NOT in the training set.\n",
    "testset = trainset.build_anti_testset()\n",
    "predictions = algo.test(testset)\n",
    "\n",
    "top_n = get_top_n(predictions, n=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'1': [('140', 10),\n",
       "  ('148', 9.268272502534833),\n",
       "  ('114', 9.251100700004795),\n",
       "  ('145', 9.133674704024061),\n",
       "  ('56', 7.420145100767315),\n",
       "  ('138', 6.472817478521407),\n",
       "  ('71', 6.137252870466734),\n",
       "  ('143', 6.110420663009521),\n",
       "  ('142', 5.790979376179711),\n",
       "  ('96', 5.762768176944148)],\n",
       " '2': [('96', 9.244926240974976),\n",
       "  ('42', 8.758231105166407),\n",
       "  ('113', 8.6033628828407),\n",
       "  ('84', 8.431754863747468),\n",
       "  ('90', 8.131098798724844),\n",
       "  ('52', 7.283908434602852),\n",
       "  ('116', 7.202755609315308),\n",
       "  ('122', 6.997517945759986),\n",
       "  ('127', 6.8010848583154075),\n",
       "  ('63', 6.7207042149052825)]}"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## filters for just the first 2 users\n",
    "dict(list(top_n.items())[:2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Next step:\n",
    "## convert .dat file to dataframe to compare and interpret ranked items"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
