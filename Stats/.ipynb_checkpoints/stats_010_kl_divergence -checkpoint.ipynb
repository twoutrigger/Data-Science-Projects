{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "20142c1e",
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic tutorial on calculating KL Divergence\n",
    "## tutorial url:\n",
    "## https://www.statology.org/kl-divergence-python/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6a302247",
   "metadata": {},
   "source": [
    "In statistics, the Kullback–Leibler (KL) divergence is a distance metric that quantifies the difference between two probability distributions.\n",
    "\n",
    "If we have two probability distributions, P and Q, we typically write the KL divergence using the notation KL(P || Q), which means “P’s divergence from Q.”\n",
    "\n",
    "We calculate it using the following formula:\n",
    "\n",
    "KL(P || Q) = ΣP(x) ln(P(x) / Q(x))\n",
    "\n",
    "If the KL divergence between two distributions is zero, then it indicates that the distributions are identical.\n",
    "\n",
    "We can use the scipy.special.rel_entr() function to calculate the KL divergence between two probability distributions in Python."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e7bd537c",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.special import rel_entr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "b07aea95",
   "metadata": {},
   "outputs": [],
   "source": [
    "#define two probability distributions\n",
    "P = [.05, .1, .2, .05, .15, .25, .08, .12]\n",
    "Q = [.3, .1, .2, .1, .1, .02, .08, .1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "82df12d7",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.589885181619163"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate (P || Q)\n",
    "sum(rel_entr(P, Q))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "86995e7d",
   "metadata": {},
   "source": [
    "Note that the units used in this calculation are known as nats, which is short for natural unit of information.\n",
    "\n",
    "Thus, we would say that the KL divergence is 0.589 nats.\n",
    "\n",
    "Also note that the KL divergence is not a symmetric metric."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "00edcfb0",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.497549319448034"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#calculate (Q || P)\n",
    "sum(rel_entr(Q, P))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ce547ef2",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
